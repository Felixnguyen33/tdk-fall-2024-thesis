{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install yfinance","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport os, datetime\nimport shutil\nfrom functools import reduce\n\nimport numpy as np\nimport pandas as pd\nfrom pandas.core.common import flatten\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import SimpleRNN, Dense\nimport tensorflow as tf\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\n\nimport yfinance as yf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"OUTPUT_FILE_NAME = ... # OUTPUT FILENAME TO SAVE\nMOVING_AVERAGE_STEPS = 14\nSEQUENCE_LEN = 128\nEPOCHS = 100 # 35, 50, or 100 is good\nBATCH_SIZE = 32\n\n# Put the ticker of the stock and its name in this list\nLIST_OF_OBJECT = [\n    (\"^GSPC\", \"S&P500\"),\n    (\"^IXIC\", \"NASDAQ\"),\n]\n\n# Transformer constant\nd_k = 256\nd_v = 256\nn_heads = 12\nff_dim = 256","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get(code: str, moving_average_steps = MOVING_AVERAGE_STEPS):\n    # Fetch history from yahoo finance\n    df = yf.Ticker(code).history(period = 'max')\n    df.drop(columns = ['Dividends', 'Stock Splits'], inplace = True, axis = 1)\n    \n    # Create missing date, fill forward method\n    start_date = df.index.min()\n    end_date = df.index.max()\n    all_dates = pd.date_range(start = start_date, end = end_date)\n    df = df.reindex(all_dates)\n    df.reset_index(inplace = True, names = ['Date'])\n    df.fillna(0, inplace = True)\n    df.sort_values('Date', inplace = True, ascending = False)\n    df.replace(to_replace = 0, method = 'ffill', inplace = True)\n    df.sort_values('Date', inplace = True)\n    \n    # Get the close prices for later use\n    plain_close_price = df['Close'].copy().values\n    \n    # Apply Moving Average and Clear NaN\n    df[['Open', 'High', 'Low', 'Close', 'Volume']] = df[['Open', 'High', 'Low', 'Close', 'Volume']].rolling(moving_average_steps).mean()\n    df.dropna(how = 'any', axis = 0, inplace = True)\n    \n    # Get the moving average values for later use\n    mva_close_price = df['Close'].copy().values\n    \n    return df, plain_close_price, mva_close_price\n\ndef draw_close_and_volume(df, df_name):\n    to_str = lambda x: x.strftime(\"%Y\")\n    \n    fig = plt.figure(figsize=(15,10))\n    st = fig.suptitle(f\"{df_name} Close Price and Volume\", fontsize=20)\n    st.set_y(0.92)\n\n    ax1 = fig.add_subplot(211)\n    ax1.plot(df['Close'], label=f'{df_name} Close Price')\n    ax1.set_xticks(range(0, df.shape[0], 1464))\n    ax1.set_xticklabels(list(map(to_str, df['Date']))[::1464])\n    ax1.set_ylabel('Close Price', fontsize=18)\n    ax1.legend(loc=\"upper left\", fontsize=12)\n\n    ax2 = fig.add_subplot(212)\n    ax2.plot(df['Volume'], label=f'{df_name} Volume')\n    ax2.set_xticks(range(0, df.shape[0], 1464))\n    ax2.set_xticklabels(list(map(to_str, df['Date']))[::1464])\n    ax2.set_ylabel('Volume', fontsize=18)\n    ax2.legend(loc=\"upper left\", fontsize=12)\n    \ndef normalize(df):\n    # Convert price and volume into daily delta values to make the series stationary.\n    # Then normalise with min-max\n\n    '''Calculate percentage change'''\n\n    df['Open'] = df['Open'].pct_change() \n    df['High'] = df['High'].pct_change()\n    df['Low'] = df['Low'].pct_change()\n    df['Close'] = df['Close'].pct_change()\n    df['Volume'] = df['Volume'].pct_change()\n\n    df.dropna(how='any', axis=0, inplace=True) # Drop all rows with NaN values\n    \n    # Get percentage changed values for later use\n    pct_close_price = df['Close'].copy().values\n    \n    ###############################################################################\n    '''Normalize price columns'''\n\n    min_return = min(df[['Open', 'High', 'Low', 'Close']].min(axis=0))\n    max_return = max(df[['Open', 'High', 'Low', 'Close']].max(axis=0))\n\n    # Min-max normalize price columns (0-1 range)\n    df['Open'] = (df['Open'] - min_return) / (max_return - min_return)\n    df['High'] = (df['High'] - min_return) / (max_return - min_return)\n    df['Low'] = (df['Low'] - min_return) / (max_return - min_return)\n    df['Close'] = (df['Close'] - min_return) / (max_return - min_return)\n\n    # Get normalized values for later use\n    normalize_close_price = df['Close'].copy().values\n    \n    ###############################################################################\n    '''Normalize volume column'''\n\n    # Get max-min encoding values for later use\n    min_volume = df['Volume'].min(axis=0)\n    max_volume = df['Volume'].max(axis=0)\n\n    # Min-max normalize volume columns (0-1 range)\n    df['Volume'] = (df['Volume'] - min_volume) / (max_volume - min_volume)\n    \n    df.replace(0, np.nan, inplace = True)\n    df.fillna(method = 'ffill', inplace = True)\n\n    return min_return, max_return, pct_close_price, normalize_close_price\n\n\"\"\" MEAN NOT NAN \"\"\"\n\ndef create_new_column(df, regex_name, method):\n    cols = df.filter(regex=regex_name)\n    Col = []\n    mnn_params = []\n    \n    if method == \"am\":\n        for _, row in cols.iterrows():\n            sum, cnt = 0, 0\n            for name in cols.columns:\n                if not np.isnan(row[name]):\n                    sum += row[name]\n                    cnt += 1\n\n            Col.append(sum / cnt)\n            mnn_params.append(cnt)\n    \n    else:\n        for _, row in cols.iterrows():\n            pro, cnt = 1, 0\n            for name in cols.columns:\n                if not np.isnan(row[name]):\n                    pro *= row[name]\n                    cnt += 1\n\n            Col.append(np.power(pro, 1 / cnt))\n            mnn_params.append(cnt)\n    \n    return pd.Series(np.transpose(np.array(Col))), mnn_params\n\ndef arithmetic_mean_not_nan(dfs):\n    method = \"am\"\n            \n    df = dfs[0]\n    for i in range(1, len(dfs)):\n        df = pd.merge(df, dfs[i], on=['Date'], how='outer', suffixes=['', f'_{i}'])\n    \n    df['Open'], _ = create_new_column(df, \"^Open\", method)\n    df['High'], _ = create_new_column(df, \"^High\", method)\n    df['Low'], _ = create_new_column(df, \"^Low\", method)\n    df['Close'], mnn_close_price = create_new_column(df, \"^Close\", method)\n    df['Volume'], _ = create_new_column(df, \"^Volume\", method)\n    \n    cols = list(filter(lambda x: '_' in x, df.columns))\n\n    df.drop(columns = cols, inplace = True)\n    df.dropna(axis = 1, how='any', inplace = True)\n    \n    return df, mnn_close_price\n\ndef geometry_mean_not_nan(dfs):\n    method = \"gm\"\n            \n    df = dfs[0]\n    for i in range(1, len(dfs)):\n        df = pd.merge(df, dfs[i], on=['Date'], how='outer', suffixes=['', f'_{i}'])\n    \n    df['Open'], _ = create_new_column(df, \"^Open\", method)\n    df['High'], _ = create_new_column(df, \"^High\", method)\n    df['Low'], _ = create_new_column(df, \"^Low\", method)\n    df['Close'], mnn_close_price = create_new_column(df, \"^Close\", method)\n    df['Volume'], _ = create_new_column(df, \"^Volume\", method)\n    \n    cols = list(filter(lambda x: '_' in x, df.columns))\n\n    df.drop(columns = cols, inplace = True)\n    df.dropna(axis = 1, how='any', inplace = True)\n    \n    return df, mnn_close_price\n\ndef split(df):\n    # Sort on date and find the rows -10% and -20% from the end\n    times = sorted(df.index.values)\n    last_10pct = sorted(df.index.values)[-int(0.1*len(times))] # Last 10% of series\n    last_20pct = sorted(df.index.values)[-int(0.2*len(times))] # Last 20% of series\n\n    # Split train, valid and test\n    df_train = df[(df.index < last_20pct)]  # Training data are 80% of total data\n    df_val = df[(df.index >= last_20pct) & (df.index < last_10pct)]\n    df_test = df[(df.index >= last_10pct)]\n\n    # Remove date column\n    df_train.drop(columns=['Date'], inplace=True)\n    df_val.drop(columns=['Date'], inplace=True)\n    df_test.drop(columns=['Date'], inplace=True)\n\n    # Convert pandas columns into arrays\n    return df_train, df_val, df_test\n\ndef draw_date_seperation(df_train, df_val, df_test, train_data, val_data, test_data):\n    fig = plt.figure(figsize=(15,12))\n    st = fig.suptitle(\"Data Separation\", fontsize=20)\n    st.set_y(0.95)\n\n    ###############################################################################\n\n    ax1 = fig.add_subplot(211)\n    ax1.plot(np.arange(train_data.shape[0]), df_train['Close'], label='Training data')\n\n    ax1.plot(np.arange(train_data.shape[0],\n                       train_data.shape[0]+val_data.shape[0]), df_val['Close'], label='Validation data')\n\n    ax1.plot(np.arange(train_data.shape[0]+val_data.shape[0],\n                       train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), df_test['Close'], label='Test data')\n    ax1.set_xlabel('Date')\n    ax1.set_ylabel('Normalized Closing Returns')\n    ax1.set_title(\"Close Price\", fontsize=18)\n    ax1.legend(loc=\"best\", fontsize=12)\n\n    ###############################################################################\n\n    ax2 = fig.add_subplot(212)\n    ax2.plot(np.arange(train_data.shape[0]), df_train['Volume'], label='Training data')\n\n    ax2.plot(np.arange(train_data.shape[0],\n                       train_data.shape[0]+val_data.shape[0]), df_val['Volume'], label='Validation data')\n\n    ax2.plot(np.arange(train_data.shape[0]+val_data.shape[0],\n                       train_data.shape[0]+val_data.shape[0]+test_data.shape[0]), df_test['Volume'], label='Test data')\n    ax2.set_xlabel('Date')\n    ax2.set_ylabel('Normalized Volume Changes')\n    ax2.set_title(\"Volume\", fontsize=18)\n    ax2.legend(loc=\"best\", fontsize=12)\n    \ndef split_data(train_data, val_data, test_data, seq_len = SEQUENCE_LEN):\n    # Training data\n    X_train, y_train = [], []\n    for i in range(seq_len, len(train_data)):\n      X_train.append(train_data[i-seq_len:i, 3]) # Chunks of training data with a length of 128 df-rows\n      y_train.append(train_data[:, 3][i]) #Value of 4th column (Close Price) of df-row 128+1\n    X_train, y_train = np.array(X_train), np.array(y_train)\n\n    ###############################################################################\n\n    # Validation data\n    X_val, y_val = [], []\n    for i in range(seq_len, len(val_data)):\n        X_val.append(val_data[i-seq_len:i, 3])\n        y_val.append(val_data[:, 3][i])\n    X_val, y_val = np.array(X_val), np.array(y_val)\n\n    ###############################################################################\n\n    # Test data\n    X_test, y_test = [], []\n    for i in range(seq_len, len(test_data)):\n        X_test.append(test_data[i-seq_len:i, 3])\n        y_test.append(test_data[:, 3][i])\n    X_test, y_test = np.array(X_test), np.array(y_test)\n\n    print('Training set shape', X_train.shape, y_train.shape)\n    print('Validation set shape', X_val.shape, y_val.shape)\n    print('Testing set shape' ,X_test.shape, y_test.shape)\n    \n    return X_train, y_train, X_val, y_val, X_test, y_test\n\nclass Time2Vector(Layer):\n  def __init__(self, seq_len, **kwargs):\n    super(Time2Vector, self).__init__()\n    self.seq_len = seq_len\n\n  def build(self, input_shape):\n    '''Initialize weights and biases with shape (batch, seq_len)'''\n    # initiate 6 matrices, 3 for ω and 3 forφ since we need aω and φ matrix for\n    # non-periodical (linear) and the periodical (sin, cosine) features.\n    self.weights_linear = self.add_weight(name='weight_linear',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n\n    self.bias_linear = self.add_weight(name='bias_linear',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n\n    self.weights_periodic_sine = self.add_weight(name='weight_periodic',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n\n    self.bias_periodic_sine = self.add_weight(name='bias_periodic',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n    \n    self.weights_periodic_cosine = self.add_weight(name='weight_periodic',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n\n    self.bias_periodic_cosine = self.add_weight(name='bias_periodic',\n                                shape=(int(self.seq_len),),\n                                initializer='uniform',\n                                trainable=True)\n\n  def call(self, x):\n    '''Calculate linear and periodic time features'''\n\n    # Exclude Volume and average across the Open, High, Low, and Close prices, resulting\n    # in the shape (batch_size, seq_len)\n    x = tf.math.reduce_mean(x[:,:,:4], axis=-1)\n\n    # calculate the non-periodic (linear) time feature and expand the dimension by 1 again ie. (batch_size, seq_len, 1)\n    time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n    time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n\n    # repeat for the periodic time feature, also resulting in the same matrix shape. (batch_size, seq_len, 1)\n    time_periodic_sine = tf.math.sin(tf.multiply(x, self.weights_periodic_sine) + self.bias_periodic_sine)\n    time_periodic_sine = tf.expand_dims(time_periodic_sine, axis=-1) # Add dimension (batch, seq_len, 1)\n    \n    # repeat for the periodic time feature, also resulting in the same matrix shape. (batch_size, seq_len, 1)\n    time_periodic_cosine = tf.math.cos(tf.multiply(x, self.weights_periodic_cosine) + self.bias_periodic_cosine)\n    time_periodic_cosine = tf.expand_dims(time_periodic_cosine, axis=-1) # Add dimension (batch, seq_len, 1)\n\n    # concatenate the linear and periodic time feature. (batch_size, seq_len, 3)\n    return tf.concat([time_linear, time_periodic_sine, time_periodic_cosine], axis=-1) # shape = (batch, seq_len, 3)\n\n  def get_config(self): # Needed for saving and loading model with custom layer\n    config = super().get_config().copy()\n    config.update({'seq_len': self.seq_len})\n    return config\n\nclass SingleAttention(Layer):\n  def __init__(self, d_k, d_v):\n    super(SingleAttention, self).__init__()\n    self.d_k = d_k\n    self.d_v = d_v\n\n  def build(self, input_shape):\n    self.query = Dense(self.d_k,\n                       input_shape=input_shape,\n                       kernel_initializer='glorot_uniform',\n                       bias_initializer='glorot_uniform')\n\n    self.key = Dense(self.d_k,\n                     input_shape=input_shape,\n                     kernel_initializer='glorot_uniform',\n                     bias_initializer='glorot_uniform')\n\n    self.value = Dense(self.d_v,\n                       input_shape=input_shape,\n                       kernel_initializer='glorot_uniform',\n                       bias_initializer='glorot_uniform')\n\n  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n    q = self.query(inputs[0])\n    k = self.key(inputs[1])\n\n    attn_weights = tf.matmul(q, k, transpose_b=True)\n    attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n    attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n\n    v = self.value(inputs[2])\n    attn_out = tf.matmul(attn_weights, v)\n    return attn_out\n\nclass MultiAttention(Layer):\n  def __init__(self, d_k, d_v, n_heads):\n    super(MultiAttention, self).__init__()\n    self.d_k = d_k\n    self.d_v = d_v\n    self.n_heads = n_heads\n    self.attn_heads = list()\n\n  def build(self, input_shape):\n    for n in range(self.n_heads):\n      self.attn_heads.append(SingleAttention(self.d_k, self.d_v))\n\n    self.linear = Dense(input_shape[0][-1],\n                        input_shape=input_shape,\n                        kernel_initializer='glorot_uniform',\n                        bias_initializer='glorot_uniform')\n\n  def call(self, inputs):\n    attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n    concat_attn = tf.concat(attn, axis=-1)\n    multi_linear = self.linear(concat_attn)\n    return multi_linear\n\nclass TransformerEncoder(Layer):\n  def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n    super(TransformerEncoder, self).__init__()\n    self.d_k = d_k\n    self.d_v = d_v\n    self.n_heads = n_heads\n    self.ff_dim = ff_dim\n    self.attn_heads = list()\n    self.dropout_rate = dropout\n\n  def build(self, input_shape):\n    self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n    self.attn_dropout = Dropout(self.dropout_rate)\n    self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n\n    self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n    self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1)\n    self.ff_dropout = Dropout(self.dropout_rate)\n    self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n\n  def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n    attn_layer = self.attn_multi(inputs)\n    attn_layer = self.attn_dropout(attn_layer)\n    attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n\n    ff_layer = self.ff_conv1D_1(attn_layer)\n    ff_layer = self.ff_conv1D_2(ff_layer)\n    ff_layer = self.ff_dropout(ff_layer)\n    ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n    return ff_layer\n\n  def get_config(self): # Needed for saving and loading model with custom layer\n    config = super().get_config().copy()\n    config.update({'d_k': self.d_k,\n                   'd_v': self.d_v,\n                   'n_heads': self.n_heads,\n                   'ff_dim': self.ff_dim,\n                   'attn_heads': self.attn_heads,\n                   'dropout_rate': self.dropout_rate})\n    return config\n\ndef create_model():\n  '''Initialize time and transformer layers'''\n  time_embedding = Time2Vector(SEQUENCE_LEN)\n  attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n  attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n  attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n  attn_layer4 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n  attn_layer5 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n\n  '''Construct model'''\n  in_seq = Input(shape=(SEQUENCE_LEN, 5))\n  x = time_embedding(in_seq)\n  x = Concatenate(axis=-1)([in_seq, x])\n  x = attn_layer1((x, x, x))\n  x = attn_layer2((x, x, x))\n  x = attn_layer3((x, x, x))\n  x = attn_layer4((x, x, x))\n  x = attn_layer5((x, x, x))\n  x = GlobalAveragePooling1D(data_format='channels_first')(x)\n  x = Dropout(0.1)(x)\n  x = Dense(64, activation='relu')(x)\n  x = Dropout(0.1)(x)\n  out = Dense(1, activation='linear')(x)\n\n  model = Model(inputs=in_seq, outputs=out)\n  model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n  return model\n\ndef evaluate(model, X_train, y_train, X_val, y_val, X_test, y_test):\n    ###############################################################################\n    '''Calculate predictions and metrics'''\n\n    #Calculate predication for training, validation and test data\n    train_pred = model.predict(X_train)\n    val_pred = model.predict(X_val)\n    test_pred = model.predict(X_test)\n\n    #Print evaluation metrics for all datasets\n    # Returned eval object contains loss and metric values\n    train_eval = model.evaluate(X_train, y_train, verbose=0)\n    val_eval = model.evaluate(X_val, y_val, verbose=0)\n    test_eval = model.evaluate(X_test, y_test, verbose=0)\n    print(' ')\n    print('Evaluation metrics')\n    print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n    print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\n    print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))\n    \n    return (train_pred, val_pred, test_pred)\n\ndef display(evaluation_result, name):\n    train_pred, val_pred, test_pred = evaluation_result\n    \n    '''Display results from predict()'''\n\n    fig = plt.figure(figsize=(15,20))\n    st = fig.suptitle(\"Transformer + TimeEmbedding Model\", fontsize=22)\n    st.set_y(0.92)\n\n    #Plot training data results\n    ax11 = fig.add_subplot(311)\n    ax11.plot(train_data[:, 3], label=f'{name} Closing Returns')\n    ax11.plot(np.arange(SEQUENCE_LEN, train_pred.shape[0]+SEQUENCE_LEN), train_pred, linewidth=3, label=f'Predicted {name} Closing Returns')\n    ax11.set_title(\"Training Data\", fontsize=18)\n    ax11.set_xlabel('Date')\n    ax11.set_ylabel(f'{name} Closing Returns')\n    ax11.legend(loc=\"best\", fontsize=12)\n\n    #Plot validation data results\n    ax21 = fig.add_subplot(312)\n    ax21.plot(val_data[:, 3], label=f'{name} Closing Returns')\n    ax21.plot(np.arange(SEQUENCE_LEN, val_pred.shape[0]+SEQUENCE_LEN), val_pred, linewidth=3, label=f'Predicted {name} Closing Returns')\n    ax21.set_title(\"Validation Data\", fontsize=18)\n    ax21.set_xlabel('Date')\n    ax21.set_ylabel(f'{name} Closing Returns')\n    ax21.legend(loc=\"best\", fontsize=12)\n\n    #Plot test data results\n    ax31 = fig.add_subplot(313)\n    ax31.plot(test_data[:, 3], label=f'{name} Closing Returns')\n    ax31.plot(np.arange(SEQUENCE_LEN, test_pred.shape[0]+SEQUENCE_LEN), test_pred, linewidth=3, label=f'Predicted {name} Closing Returns')\n    ax31.set_title(\"Test Data\", fontsize=18)\n    ax31.set_xlabel('Date')\n    ax31.set_ylabel(f'{name} Closing Returns')\n    ax31.legend(loc=\"best\", fontsize=12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def adjust_list_lengths(target_list, *lists):\n    # Get the length of the target list\n    target_length = len(target_list)\n    \n    # Placeholder for extending lists that are too short\n    placeholder = 1\n    \n    # Create a list to store the adjusted lists\n    adjusted_lists = []\n    \n    # Iterate through the provided lists\n    for lst in lists:\n        # Calculate the length difference\n        length_difference = len(lst) - target_length\n        \n        if length_difference > 0:\n            # List is too long: trim from the beginning\n            adjusted_list = lst[length_difference:]\n        elif length_difference < 0:\n            # List is too short: prepend placeholders to the beginning\n            adjusted_list = [placeholder] * abs(length_difference) + lst\n        else:\n            # List is already the right length\n            adjusted_list = lst\n        \n        # Add the adjusted list to the list of adjusted lists\n        adjusted_lists.append(adjusted_list)\n    \n    return adjusted_lists\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score\n\ndef calculate_rmse(real_prices, predicted_prices):\n    mse = mean_squared_error(real_prices, predicted_prices)\n    return np.sqrt(mse)\n\ndef calculate_mse(real_prices, predicted_prices):\n    return mean_squared_error(real_prices, predicted_prices)\n\ndef calculate_mape(real_prices, predicted_prices):\n    real_prices, predicted_prices = np.array(real_prices), np.array(predicted_prices)\n    mape = np.mean(np.abs((real_prices - predicted_prices) / real_prices)) * 100\n    return mape\n\ndef calculate_mae(real_prices, predicted_prices):\n    return mean_absolute_error(real_prices, predicted_prices)\n\ndef calculate_r2(real_prices, predicted_prices):\n    TSS = np.sum((np.array(real_prices) - np.mean(np.array(real_prices))) ** 2)\n\n    # Calculate RSS (Residual Sum of Squares)\n    RSS = np.sum((np.array(real_prices) - np.array(predicted_prices)) ** 2)\n\n    # Calculate R²\n    R2 = 1 - (RSS / TSS)\n    return R2\n\ndef stat(real_prices, predicted_prices):\n    print(\"RMSE:\", calculate_rmse(real_prices, predicted_prices))\n    print(\"MSE:\", calculate_mse(real_prices, predicted_prices))    \n    print(\"MAPE:\", calculate_mape(real_prices, predicted_prices))    \n    print(\"MAE:\", calculate_mae(real_prices, predicted_prices))    \n    print(\"R2:\", calculate_r2(real_prices, predicted_prices))    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***TRANING PROCESS STARTS FROM HERE***","metadata":{}},{"cell_type":"code","source":"# Single feature\nif len(LIST_OF_OBJECt) == 1:\n    df = get('XOM')[0]\n    normalize(df)\n    \n# Multiple features\nelse:\n    dfs = [get(code)[0] for code, _ in LIST_OF_OBJECT]\n\n    for df, (_, name) in zip(dfs, LIST_OF_OBJECT):\n        draw_close_and_volume(df, name)\n\n    for df in dfs:\n        normalize(df)\n\n    df, mnn_params = geometry_mean_not_nan(dfs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Split the data sequentially into 80% train, 10% valid and 10% test\n\n###############################################################################\n'''Create training, validation and test split'''\ndf_train, df_val, df_test = split(df)\ntrain_data, val_data, test_data = df_train.values, df_val.values, df_test.values\n\nprint('Training data shape: {}'.format(train_data.shape))\nprint('Validation data shape: {}'.format(val_data.shape))\nprint('Test data shape: {}'.format(test_data.shape))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"draw_date_seperation(df_train, df_val, df_test, train_data, val_data, test_data)\nX_train, y_train, X_val, y_val, X_test, y_test = split_data(train_data, val_data, test_data)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = create_model()\nmodel.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = tf.keras.callbacks.ModelCheckpoint(f'{OUTPUT_FILE_NAME}.keras',\n                                              monitor='val_loss',\n                                              save_best_only=True, verbose=1)\n\nhistory = model.fit(X_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    callbacks=[callback],\n                    validation_data=(X_val, y_val))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate_result = evaluate(model, X_train, y_train, X_val, y_val, X_test, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(evaluate_result, \"Mix Data\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig = plt.figure(figsize=(15,20))\nst = fig.suptitle(\"Transformer + TimeEmbedding Model Metrics\", fontsize=22)\nst.set_y(0.92)\n\n#Plot model loss\nax1 = fig.add_subplot(311)\nax1.plot(history.history['loss'], label='Training loss (MSE)')\nax1.plot(history.history['val_loss'], label='Validation loss (MSE)')\nax1.set_title(\"Model loss\", fontsize=18)\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss (MSE)')\nax1.legend(loc=\"best\", fontsize=12)\n\n#Plot MAE\nax2 = fig.add_subplot(312)\nax2.plot(history.history['mae'], label='Training MAE')\nax2.plot(history.history['val_mae'], label='Validation MAE')\nax2.set_title(\"Model metric - Mean average error (MAE)\", fontsize=18)\nax2.set_xlabel('Epoch')\nax2.set_ylabel('Mean average error (MAE)')\nax2.legend(loc=\"best\", fontsize=12)\n\n#Plot MAPE\nax3 = fig.add_subplot(313)\nax3.plot(history.history['mape'], label='Training MAPE')\nax3.plot(history.history['val_mape'], label='Validation MAPE')\nax3.set_title(\"Model metric - Mean average percentage error (MAPE)\", fontsize=18)\nax3.set_xlabel('Epoch')\nax3.set_ylabel('Mean average percentage error (MAPE)')\nax3.legend(loc=\"best\", fontsize=12)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***FURTHER EVALUATION***","metadata":{}},{"cell_type":"code","source":"target, plain_close_price, mva_close_price = get('...') # Place the target stock's ticker to evaluate\nm, M, pct_close_price, normalize_close_price = normalize(target)\nplain_close_price = plain_close_price.tolist()\nmva_close_price = mva_close_price.tolist()\npct_close_price = pct_close_price.tolist()\nnormalize_close_price = normalize_close_price.tolist()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\" Predict real target \"\"\"\n\ndata = target.drop(columns = ['Date'])\ndata = data.values\nx = []\ny = []\nfor i in range(SEQUENCE_LEN, len(data)):\n    x.append(data[i - SEQUENCE_LEN:i])\n    y.append(data[:, 3][i]) #Value of 4th column (Close Price) of df-row 128+1\nx, y = np.array(x), np.array(y)\n\nx = model.predict(x)\nx = x.tolist()\nx = list(flatten(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,20))\nplt.plot(y)\nplt.plot(x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Display statistic base on RMSE, MSE, MAPE, MAE, R2\nstat(y, x)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The model may predict some values that greater than 1 (it is rare, but if it happened, we need to handle)\nnormalize_predict_close_price = []\nfor i in range(len(x)):\n    if x[i] > 1:\n        normalize_predict_close_price.append(1 / x[i])\n    else:\n        normalize_predict_close_price.append(x[i])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Concatinating back the first 128 values that we have lost due to splitting data\nnormalize_predict_close_price = normalize_close_price[:128] + normalize_predict_close_price\nplt.plot(normalize_predict_close_price)\nplt.plot(normalize_close_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"***Reverse to find back values***","metadata":{}},{"cell_type":"code","source":"# \"Find back\" predicted percentage change values\npct_predict_close_price = []\nfor val in normalize_predict_close_price:\n    pct_predict_close_price.append(val * (M - m) + m)\n\n# And display it with the real one\nplt.plot(pct_predict_close_price)\nplt.plot(pct_close_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"Find back\" moving average values\nmva_predict_close_price = [mva_close_price[0]]\nfor i, val in enumerate(pct_predict_close_price):\n    mva_predict_close_price.append(mva_close_price[i] * (1 + val))\n    \n# And display it with the real one\nplt.figure(figsize=(15,20))\nplt.plot(mva_predict_close_price)\nplt.plot(mva_close_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the statistic of our reverse engineering with moving average prices\nstat(mva_close_price, mva_predict_close_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# \"Find back\" real values\nplain_predict_close_price = plain_close_price[:MOVING_AVERAGE_STEPS - 1]\n\nfor i, val in enumerate(mva_predict_close_price):\n    plain_predict_close_price.append(\n        abs(val * MOVING_AVERAGE_STEPS - sum(plain_close_price[i :MOVING_AVERAGE_STEPS - 1 + i]))\n    )\n\n# And display it with the real one\nplt.figure(figsize=(20,20))\nplt.plot(plain_predict_close_price)\nplt.plot(plain_close_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the statistic of our reverse engineering with real close prices\nstat(plain_close_price, plain_predict_close_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def convert_to_trend(prices):\n    trend = []\n    # Iterate through the list starting from the second element\n    for i in range(1, len(prices)):\n        # Compare the current price with the previous price\n        if prices[i] >= prices[i - 1]:\n            trend.append(1)  # Increasing trend\n        else:\n            trend.append(0)  # No change in trend\n    return trend","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plain_trend = convert_to_trend(mva_close_price)\npredict_trend = convert_to_trend(mva_predict_close_price)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ndef evaluate_trend_predictions(real_trends, predicted_trends):\n    # Calculate accuracy\n    accuracy = accuracy_score(real_trends, predicted_trends)\n    \n    # Calculate precision, recall, and F1-score for each class (0, 1)\n    precision, recall, f1_score, _ = precision_recall_fscore_support(\n        real_trends, predicted_trends, labels=[0, 1], zero_division=0\n    )\n    \n    # Print the results\n    print(f\"Accuracy: {accuracy:.4f}\")\n    print(f\"Precision for 0, 1: {precision}\")\n    print(f\"Recall for 0, 1: {recall}\")\n    print(f\"F1-score for 0, 1: {f1_score}\")\n\n    # Return the metrics as a dictionary\n    metrics = {\n        'accuracy': accuracy,\n        'precision': precision,\n        'recall': recall,\n        'f1_score': f1_score\n    }\n    \n    return metrics","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"c = evaluate_trend_predictions(plain_trend, predict_trend)","metadata":{},"execution_count":null,"outputs":[]}]}