\chapter{Introduction}
\label{ch:intro}

Time series forecasting is challenging, especially in the financial industry \cite{pedersen2019efficiently}. It involves statistically understanding complex linear, and non-linear interactions within historical data to predict the future. Traditional statistical approaches commonly adopt linear regression, exponential smoothing \cite{gardner1985forecasting}, and auto regression model \cite{SEMENOGLOU20211072}.
With the advances in deep learning, recent works are heavily invested in ensemble models and sequence-to-sequence modeling such as RNN (recurrent neural networks), and Long Short-Term Memory \cite{6795963}. However, the primary drawback of these methods is that the RNN family struggles to capture extremely long-term dependencies \cite{DBLP:journals/corr/abs-1907-00235}. A well-known sequence-to-sequence model called Transformer \cite{DBLP:journals/corr/VaswaniSPUJGKP17} has achieved great success in NLP, especially LLM like ChatGPT, Gemini. Different from RNN-based modes, Transformer employs a multi-head self-attention mechanism to learn the relationship among different positions globally. For example, in terms of the tech industry, such as the layoff of employees in 2023, first begin with Meta, then move to Google, Microsoft, and Apple. Then the stock price of those companies is almost down or up at the same time. Or in terms of social network companies. In other industries like oil, it makes people suspect the current market and sell all stocks they have.